## 1. 决策树


![决策树.png](http://upload-images.jianshu.io/upload_images/4097708-d49db16e11942935.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

- 决策树（decision tree）是一个树结构（可以是二叉树或非二叉树）。其每个非叶节点表示一个特征属性上的测试，每个分支代表这个特征属性在某个值域上的输出，而每个叶节点存放一个类别。使用决策树进行决策的过程就是从根节点开始，测试待分类项中相应的特征属性，并按照其值选择输出分支，直到到达叶子节点，将叶子节点存放的类别作为决策结果。

- 当构建决策树的时候，上层节点要有最大的信息增益，如果叶子结点不纯净，则选出现次数多的那个

## 2. 随机森林 RF

- [随机森林](http://blog.csdn.net/holybin/article/details/25653597)

- 随机森林由许多的决策树组成，因为这些决策树的形成采用了随机的方法，所以叫做随机森林。

- 当测试数据进入随机森林时，其实就是让每一颗决策树进行分类看看这个样本应该属于哪一类，最后取所有决策树中分类结果最多的那类为最终的结果（每棵树的权重要考虑进来）。所有的树训练都是使用同样的参数，但是训练集是不同的，分类器的错误估计采用的是oob（out of bag）的办法。

- 随机森林可以既可以处理属性为离散值的量，如ID3算法，也可以处理属性为连续值的量，比如C4.5算法。

## 3. kNN

- k-近邻(kNN，k-Nearest Neighbors)算法是一种基于实例的分类方法。该方法就是找出与未知样本x距离最近的k个训练样本，看这k个样本中多数属于哪一类，就把x归为那一类。k-近邻方法是一种懒惰学习方法，它存放样本，直到需要分类时才进行分类，如果样本集比较复杂，可能会导致很大的计算开销，因此无法应用到实时性很强的场合

## 4. 朴素贝叶斯

- 就好比这么个道理，你在街上看到一个黑人，我问你你猜这哥们哪里来的，你十有八九猜非洲。为什么呢？因为黑人中非洲人的比率最高，当然人家也可能是美洲人或亚洲人，但在没有其它可用信息下，我们会选择条件概率最大的类别，这就是朴素贝叶斯的思想基础。

-       第一阶段——准备工作阶段，这个阶段的任务是为朴素贝叶斯分类做必要的准备，主要工作是根据具体情况确定特征属性，并对每个特征属性进行适当划分，然后由人工对一部分待分类项进行分类，形成训练样本集合。这一阶段的输入是所有待分类数据，输出是特征属性和训练样本。这一阶段是整个朴素贝叶斯分类中唯一需要人工完成的阶段，其质量对整个过程将有重要影响，分类器的质量很大程度上由特征属性、特征属性划分及训练样本质量决定。

- 第二阶段——分类器训练阶段，这个阶段的任务就是生成分类器，主要工作是计算每个类别在训练样本中的出现频率及每个特征属性划分对每个类别的条件概率估计，并将结果记录。其输入是特征属性和训练样本，输出是分类器。这一阶段是机械性阶段，根据前面讨论的公式可以由程序自动计算完成。

- 第三阶段——应用阶段。这个阶段的任务是使用分类器对待分类项进行分类，其输入是分类器和待分类项，输出是待分类项与类别的映射关系。这一阶段也是机械性阶段，由程序完成。

## 5. linear regression

- 最优解，突优化
- 做拟合，x对y
- 找到那条线

## 6. k-means

- unsupervised learning
- EM:k-means是特例
- EML

## 7. 高斯混合模型

- 一堆数据，两个高斯组成了现在的数据。然后去分离出来这两个峰

## 8. Bias Varience Tradeoff

- 模型简单，varaince低，不容易overfit，bias
- 复杂模型，bias低，variance高

## 9. SVM

- 分类，与逻辑回归区别是，它即使分类对了，还要使得分类的margin最平均。

- 点到超平面的距离分布要好

## 10. Kernel SVM
- 线性不可分
- 大量数据也没啥用

## 11. boosting

- 弱分类器合成强分类器，每个可以做到80%，可以合起来分90%
- training data，弱分类器
- Boosting这其实思想相当的简单，大概是，对一份数据，建立M个模型（比如分类），一般这种模型比较简单，称为弱分类器(weak learner)每次分类都将上一次分错的数据权重提高一点再进行分类，这样最终得到的分类器在测试数据与训练数据上都可以得到比较好的成绩。

- LDA, PCA：都是主成分分析，LDA是为了作分类，而PCA是为了预处理

## 12. 神经网络

## 13. 总结

- 如果是小训练集，高偏差/低方差的分类器（例如，朴素贝叶斯NB）要比低偏差/高方差大分类的优势大（例如，KNN），因为后者会发生过拟合（overfiting）

- 复杂模型需要大量训练数据，不然容易过拟合。在一些特征明确，或者概率明确的时候，可以使用简单的模型，如朴素贝叶斯，逻辑回归。kNN在样本数据数量不一样的情况下效果不好。

- 决策树容易发生过拟合

- 线性回归不能解决非线性问题，SVM也不行，但是kernelSVM则可以。

- 朴素贝叶斯需要决定一些先验概率（这些先验概率往往是不知道的）
